{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from ultralytics import settings\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "import yaml\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'settings_version': '0.0.4', 'datasets_dir': 'D:\\\\Tom\\\\GitHub\\\\Third Year Project\\\\datasets', 'weights_dir': 'D:\\\\Tom\\\\GitHub\\\\Third Year Project\\\\Intruder-Aircraft-Detection\\\\weights', 'runs_dir': 'D:\\\\Tom\\\\GitHub\\\\Third Year Project\\\\Intruder-Aircraft-Detection\\\\runs', 'uuid': 'fdb5c10788ffaa41a9047dc764dd8a0a3287d6bec12c0c66234ac745c0366efa', 'sync': True, 'api_key': '', 'openai_api_key': '', 'clearml': True, 'comet': True, 'dvc': True, 'hub': True, 'mlflow': True, 'neptune': True, 'raytune': True, 'tensorboard': True, 'wandb': True}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Ultralytics settings\n",
    "print(settings) \n",
    "\n",
    "# GPU Utilisation\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base paths for the images and labels\n",
    "train_images_path = 'datasets/AVOIDDS/images/train'\n",
    "train_labels_path = 'datasets/AVOIDDS/labels/train'\n",
    "val_images_path = 'datasets/AVOIDDS/images/valid'\n",
    "val_labels_path = 'datasets/AVOIDDS/labels/valid'\n",
    "\n",
    "# Load the metadata\n",
    "metadata_path = 'datasets/AVOIDDS/metadata.json'\n",
    "with open(metadata_path, 'r') as file:\n",
    "    metadata = json.load(file)\n",
    "\n",
    "# Function to create a DataFrame from images and labels\n",
    "def create_dataframe(images_path, labels_path, metadata):\n",
    "\n",
    "    # List all files in the directories\n",
    "    image_files = [f for f in sorted(os.listdir(images_path)) if f.endswith('.jpg')]\n",
    "    label_files = [f for f in sorted(os.listdir(labels_path)) if f.endswith('.txt')]\n",
    "    \n",
    "    # Create tempory DataFrame so that final dataframe is in correct order\n",
    "    temp_df = pd.DataFrame({\n",
    "        'image_path': [str(images_path + '/' + file) for file in image_files],\n",
    "        'label_path': [str(labels_path + '/' + file) for file in label_files],\n",
    "    })\n",
    "\n",
    "    # Extract image indices to match with metadata\n",
    "    df = pd.DataFrame()\n",
    "    df['imageID'] = temp_df['image_path'].apply(lambda x: int(os.path.splitext(os.path.basename(x))[0]))\n",
    "\n",
    "    # Add image and label paths to final dataframe\n",
    "    df['image_path'] = temp_df['image_path']\n",
    "    df['label_path'] = temp_df['label_path']\n",
    " \n",
    "    # Add metadata to each image entry\n",
    "    for key, value in metadata.items():\n",
    "        if '.' in key:  # Key represents a range\n",
    "            start, end = map(int, key.split('.'))\n",
    "            df.loc[df['imageID'].between(start, end), 'metadata'] = json.dumps(value)\n",
    "\n",
    "    # Convert the JSON strings in 'metadata' to dictionaries\n",
    "    df['metadata'] = df['metadata'].apply(json.loads)\n",
    "\n",
    "    # Expand the 'metadata' column into separate columns\n",
    "    metadata_df = pd.json_normalize(df['metadata'])\n",
    "    \n",
    "    # Concatenate the expanded metadata back to the original DataFrame\n",
    "    full_df = pd.concat([df.drop(['metadata'], axis=1), metadata_df], axis=1)\n",
    "\n",
    "    return full_df\n",
    "\n",
    "# Create the DataFrames for the train and validation sets\n",
    "train_df = create_dataframe(train_images_path, train_labels_path, metadata)\n",
    "valid_df = create_dataframe(val_images_path, val_labels_path, metadata)\n",
    "\n",
    "#train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create sub datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes all files and folders from dataset directory\n",
    "def clear_directory(dir_path):    \n",
    "    for item in os.listdir(dir_path):\n",
    "        item_path = os.path.join(dir_path, item)\n",
    "        if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "            os.unlink(item_path)\n",
    "        elif os.path.isdir(item_path):\n",
    "            shutil.rmtree(item_path)\n",
    "\n",
    "\n",
    "## creates subsets of main dataset\n",
    "def create_dataset(dataset_name, filtered_train_df, filtered_valid_df, class_names=['aircraft'], dataset_dir=\"datasets/\"):\n",
    "    new_dataset_dir = Path(dataset_dir) / dataset_name\n",
    "    images_dir = Path(new_dataset_dir) / 'images'\n",
    "    labels_dir = Path(new_dataset_dir) / 'labels'\n",
    "\n",
    "    # Clear directories if they exist, to overwrite the dataset\n",
    "    for subdir in ['train', 'valid']:\n",
    "        img_subdir = images_dir / subdir\n",
    "        label_subdir = labels_dir / subdir\n",
    "        if img_subdir.exists():\n",
    "            clear_directory(img_subdir)\n",
    "        else:\n",
    "            img_subdir.mkdir(parents=True, exist_ok=True)\n",
    "        if label_subdir.exists():\n",
    "            clear_directory(label_subdir)\n",
    "        else:\n",
    "            label_subdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create directories\n",
    "    for subdir in ['train', 'valid']:\n",
    "        (images_dir / subdir).mkdir(parents=True, exist_ok=True)\n",
    "        (labels_dir / subdir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def copy_file(src, dest):\n",
    "        shutil.copy2(src, dest)\n",
    "\n",
    "    def copy_files_concurrently(df, img_dest_dir, label_dest_dir):\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            # Prepare futures for image and label copying\n",
    "            futures = [executor.submit(copy_file, row['image_path'], img_dest_dir / f\"{Path(row['image_path']).name}\") for _, row in df.iterrows()]\n",
    "            futures += [executor.submit(copy_file, row['label_path'], label_dest_dir / f\"{Path(row['label_path']).name}\") for _, row in df.iterrows()]\n",
    "            \n",
    "            # Initialize progress bar\n",
    "            pbar = tqdm(total=len(futures), desc='Copying files')\n",
    "            for future in as_completed(futures):\n",
    "                # Update progress bar upon task completion\n",
    "                pbar.update(1)\n",
    "            pbar.close()\n",
    "\n",
    "    print(\"Copying training files:\")\n",
    "    copy_files_concurrently(filtered_train_df, images_dir / 'train', labels_dir / 'train')\n",
    "    print(\"Copying validation files:\")\n",
    "    copy_files_concurrently(filtered_valid_df, images_dir / 'valid', labels_dir / 'valid')\n",
    "\n",
    "    # Construct the YAML content with the desired structure\n",
    "    yaml_content = {\n",
    "        'path': str(f'../{new_dataset_dir}').replace('\\\\', '/'),  # Ensuring forward slashes\n",
    "        'train': str('images/train'),\n",
    "        'val': str('images/valid'),\n",
    "        'names': {index: name for index, name in enumerate(class_names)}\n",
    "    }  \n",
    "\n",
    "    yaml_path = new_dataset_dir / f\"{dataset_name}.yaml\"\n",
    "    with open(yaml_path, 'w') as file:\n",
    "        yaml.dump(yaml_content, file, sort_keys=False)\n",
    "\n",
    "    print(f\"Dataset '{dataset_name}' created at {dataset_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying training files:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1567e416a60433fae7dc3e4a3ee8281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Copying files:   0%|          | 0/43200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying validation files:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707a3259ee224446952e1acbd59689ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Copying files:   0%|          | 0/4800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'dataset' created at datasets/cessna_only\n"
     ]
    }
   ],
   "source": [
    "# test dataset creation\n",
    "\n",
    "#test_train_df = train_df[train_df['image_path'] == 'datasets/AVOIDDS/images/train/1.jpg']\n",
    "#test_valid_df = valid_df[valid_df['image_path'] == 'datasets/AVOIDDS/images/valid/900.jpg']\n",
    "#\n",
    "#dataset_name = 'test'\n",
    "\n",
    "test_train_df = train_df[train_df['ac'] == 'Cessna Skyhawk']\n",
    "test_valid_df = valid_df[valid_df['ac'] == 'Cessna Skyhawk']\n",
    "\n",
    "dataset_name = 'dataset'\n",
    "\n",
    "create_dataset(dataset_name, test_train_df, test_valid_df, dataset_dir='datasets/cessna_only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts class names and bboxes from all objects in label\n",
    "def load_yolo_labels(label_path):\n",
    "    with open(label_path, 'r') as file:\n",
    "        labels = [line.strip().split() for line in file.readlines()]\n",
    "        bboxes = [list(map(float, label[1:])) for label in labels]\n",
    "        class_labels = [int(label[0]) for label in labels]\n",
    "    return bboxes, class_labels\n",
    "\n",
    "# takes class names and augmented bbox and converts into yolo label format\n",
    "def format_yolo_label(class_labels, augmented_bboxes):\n",
    "    label_str = \"\"\n",
    "    for class_label, bbox in zip(class_labels, augmented_bboxes):\n",
    "        label_str += f\"{class_label} \" + \" \".join(f\"{x:.6f}\" for x in bbox) + \"\\n\"\n",
    "    return label_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## horizontal/vertical flip\n",
    "\n",
    "def augment_flip(image_path, label_path, orientation, p=1.0):\n",
    "    # Load image - openCV\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Load labels\n",
    "    bboxes, class_labels = load_yolo_labels(label_path)\n",
    "    \n",
    "    # Define the augmentation based on the orientation parameter\n",
    "    if orientation == 'h':\n",
    "        flip = A.HorizontalFlip(p=p)\n",
    "    elif orientation == 'v':\n",
    "        flip = A.VerticalFlip(p=p)\n",
    "    else:\n",
    "        raise ValueError(\"Orientation must be 'h' or 'v'\")\n",
    "    \n",
    "    transform = A.Compose([\n",
    "        flip,\n",
    "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "    \n",
    "    # Apply transformation\n",
    "    transformed = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "    augmented_image = transformed['image']\n",
    "    augmented_bboxes = transformed['bboxes']\n",
    "    \n",
    "    # YOLO formatted label: [class_id, x_center, y_center, width, height]\n",
    "    augmented_label = format_yolo_label(class_labels, augmented_bboxes)\n",
    "    \n",
    "    return augmented_image, augmented_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rotation\n",
    "\n",
    "def augment_rotation(image_path, label_path, angle, p=1.0):\n",
    "   \n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Load labels\n",
    "    bboxes, class_labels = load_yolo_labels(label_path)\n",
    "    \n",
    "    # Define the augmentation with rotation\n",
    "    transform = A.Compose([\n",
    "        A.Rotate(limit=(angle, angle), p=p, border_mode=cv2.BORDER_CONSTANT),\n",
    "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "    \n",
    "    # Apply transformation\n",
    "    transformed = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "    augmented_image = transformed['image']\n",
    "    augmented_bboxes = transformed['bboxes']\n",
    "    \n",
    "    # Convert augmented bboxes and class labels back to YOLO format\n",
    "    augmented_label = format_yolo_label(class_labels, augmented_bboxes)\n",
    "    \n",
    "    return augmented_image, augmented_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## contrast and brightness\n",
    "\n",
    "# Alpha - contrast control (1.0-3.0)\n",
    "# Beta - brightness control (-100 to 100)\n",
    "def augment_brightness_contrast(image_path, alpha=1.0, beta=0):\n",
    "    \n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Apply brightness and contrast adjustment\n",
    "    augmented_image = np.clip(alpha * image.astype(np.float32) + beta, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return augmented_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## histogram equalisation with CLAHE\n",
    "\n",
    "def augment_histogram_equalization(image_path, p=1.0):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Define the augmentation\n",
    "    transform = A.Compose([\n",
    "        A.CLAHE(p),\n",
    "    ])\n",
    "    \n",
    "    # Apply the augmentation\n",
    "    transformed = transform(image=image)\n",
    "    augmented_image = transformed['image']\n",
    "    \n",
    "    return augmented_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## white balancing - gray word algorithm\n",
    "\n",
    "def augment_white_balance(image_path):\n",
    "  \n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Calculate the mean of each channel\n",
    "    mr = np.mean(image[:, :, 0])\n",
    "    mg = np.mean(image[:, :, 1])\n",
    "    mb = np.mean(image[:, :, 2])\n",
    "    \n",
    "    # Calculate the overall mean\n",
    "    mgray = (mr + mg + mb) / 3\n",
    "    \n",
    "    # Scale the channels based on the Gray World assumption\n",
    "    image[:, :, 0] = np.clip(image[:, :, 0] * (mgray / mr), 0, 255)\n",
    "    image[:, :, 1] = np.clip(image[:, :, 1] * (mgray / mg), 0, 255)\n",
    "    image[:, :, 2] = np.clip(image[:, :, 2] * (mgray / mb), 0, 255)\n",
    "    \n",
    "    augmented_image = image.astype(np.uint8)\n",
    "    return augmented_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sharpening \n",
    "\n",
    "def augment_sharpen(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Kernels from literature research\n",
    "    kernel_1 = np.array([[0, -1, 0],\n",
    "                         [-1, 5, -1],\n",
    "                         [0, -1, 0]], dtype=np.float32)\n",
    "    \n",
    "    kernel_2 = np.array([[-1, -2, -1],\n",
    "                         [-2, 13, -2],\n",
    "                         [-1, -2, -1]], dtype=np.float32)\n",
    "\n",
    "    kernel_2 = np.array([[-1, -2, -1],\n",
    "                         [-2, 16, -2],\n",
    "                         [-1, -2, -1]], dtype=np.float32)\n",
    "    \n",
    "    # Apply the sharpening kernel to the image\n",
    "    sharpened_image = cv2.filter2D(image, -1, kernel_1)\n",
    "    \n",
    "    return sharpened_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## guassian noise\n",
    "\n",
    "def augment_gaussian_noise(image_path, var_limit=(10.0, 50.0)):\n",
    "   \n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Define the augmentation\n",
    "    transform = A.Compose([\n",
    "        A.GaussNoise(var_limit=var_limit, mean=0, p=1.0),\n",
    "    ])\n",
    "    \n",
    "    # Apply the augmentation\n",
    "    transformed = transform(image=image)\n",
    "    augmented_image = transformed['image']\n",
    "    \n",
    "    return augmented_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## zoom in \n",
    "\n",
    "# zoom factor: No zoom = 1, Full zoom (bounding box takes up entire picture) = 10\n",
    "def augment_zoom(image_path, label_path, zoom_factor=1.5):\n",
    "\n",
    "    # adjust zoom factor so that 1 = no zoom\n",
    "    zoom_factor = zoom_factor / 10\n",
    "\n",
    "    # Load image and labels\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    bboxes, class_labels = load_yolo_labels(label_path)\n",
    "\n",
    "    # Calculate the crop dimensions based on the bounding box and zoom factor\n",
    "    x_center, y_center, bbox_width, bbox_height = bboxes[0]  # Assuming one object\n",
    "    x_center *= image.shape[1]  # Convert from relative to absolute coordinates\n",
    "    y_center *= image.shape[0]\n",
    "    bbox_width *= image.shape[1]\n",
    "    bbox_height *= image.shape[0]\n",
    "\n",
    "    # Define the crop dimensions\n",
    "    crop_width = int(bbox_width / zoom_factor)\n",
    "    crop_height = int(bbox_height / zoom_factor)\n",
    "    \n",
    "    # Calculate the crop coordinates\n",
    "    x_min = max(0, int(x_center - crop_width / 2))\n",
    "    y_min = max(0, int(y_center - crop_height / 2))\n",
    "    x_max = min(image.shape[1], int(x_center + crop_width / 2))\n",
    "    y_max = min(image.shape[0], int(y_center + crop_height / 2))\n",
    "\n",
    "    # Define Albumentations transform for cropping and resizing\n",
    "    transform = A.Compose([\n",
    "        A.Crop(x_min=x_min, y_min=y_min, x_max=x_max, y_max=y_max, p=1.0),\n",
    "        A.Resize(height=image.shape[0], width=image.shape[1], p=1.0)\n",
    "    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "    \n",
    "    # Apply transformation\n",
    "    transformed = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "    transformed_image = transformed['image']\n",
    "    transformed_bboxes = transformed['bboxes']\n",
    "    \n",
    "    # The transformed bboxes are already in YOLO format\n",
    "    augmented_label = format_yolo_label(class_labels, transformed_bboxes)\n",
    "\n",
    "    return transformed_image, augmented_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tester method to overlay bboxes on images\n",
    "\n",
    "def overlay_bbox(image_path, label_path):\n",
    "\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Load labels\n",
    "    bboxes, class_labels = load_yolo_labels(label_path)\n",
    "    \n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    height, width, _ = image.shape\n",
    "    for bbox, class_label in zip(bboxes, class_labels):\n",
    "        x_center, y_center, bbox_width, bbox_height = bbox\n",
    "        x_min = (x_center - bbox_width / 2) * width\n",
    "        y_min = (y_center - bbox_height / 2) * height\n",
    "        \n",
    "        rect = patches.Rectangle((x_min, y_min), bbox_width * width, bbox_height * height,\n",
    "                                 linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        plt.text(x_min, y_min - 2, str(class_label), color='red', fontsize=10, \n",
    "                 bbox=dict(facecolor='white', alpha=0.5, edgecolor='red', boxstyle='round'))\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to save image\n",
    "def save_image(path, image):\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    \n",
    "\n",
    "    # Convert from RGB to BGR\n",
    "    image_to_save = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Save the image\n",
    "    cv2.imwrite(path, image_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to save label\n",
    "def save_label(path, contents):\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    \n",
    "    # Now, save the file\n",
    "    with open(path, 'w') as label_file:\n",
    "        label_file.write(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block to test aug methods\n",
    "\n",
    "image = '1'\n",
    "dataset = 'train'\n",
    "aug_type = 'flip'\n",
    "\n",
    "test_image_path = f'datasets/test/images/{dataset}/{image}.jpg'\n",
    "test_label_path = f'datasets/test/labels/{dataset}/{image}.txt'\n",
    "\n",
    "aug_image_path = f'datasets/test-aug/images/{aug_type}/{image}.jpg'\n",
    "aug_label_path = f'datasets/test-aug/labels/{aug_type}/{image}.txt'\n",
    "\n",
    "aug_image, aug_label = augment_rotation(test_image_path, test_label_path, 45)\n",
    "\n",
    "save_image(aug_image_path, aug_image)\n",
    "save_label(aug_label_path, aug_label)\n",
    "\n",
    "overlay_bbox(aug_image_path, aug_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create augmented datasets\n",
    "\n",
    "def copy_file(src, dst):\n",
    "    # Create the destination directory if it doesn't exist.\n",
    "    os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "    # Copy the file\n",
    "    shutil.copy2(src, dst)\n",
    "\n",
    "def copy_dataset_in_parallel(src, dst):\n",
    "    # List all files in the original dataset directory and its subdirectories\n",
    "    all_files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(src) for f in filenames]\n",
    "    # Calculate the relative paths\n",
    "    relative_paths = [os.path.relpath(file, src) for file in all_files]\n",
    "    \n",
    "    # Create the destination directory\n",
    "    os.makedirs(dst, exist_ok=True)\n",
    "    \n",
    "    # Start a progress bar\n",
    "    pbar = tqdm(total=len(all_files), desc='Copying dataset')\n",
    "\n",
    "    # Copy files in parallel\n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        # Prepare the future copy operations\n",
    "        futures = {executor.submit(copy_file, src_file, os.path.join(dst, rel_path)): rel_path for src_file, rel_path in zip(all_files, relative_paths)}\n",
    "        # As each copy operation completes, update the progress bar\n",
    "        for future in futures:\n",
    "            future.result()  # Wait for each future to complete\n",
    "            pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "\n",
    "def apply_augmentation(image_path, augmented_dataset_path, method_name, details):\n",
    "    label_path = image_path.replace('images/train', 'labels/train').replace('.jpg', '.txt')\n",
    "    imageID = os.path.basename(image_path)\n",
    "    aug_image_filename = imageID.replace(\".jpg\", f\"-{method_name}.jpg\")\n",
    "    aug_label_filename = imageID.replace(\".jpg\", f\"-{method_name}.txt\")\n",
    "    \n",
    "    # Determine which augmentation function to call based on the method name\n",
    "    if method_name == 'flip':\n",
    "        augmented_image, augmented_label = augment_flip(image_path, label_path, **details['parameters'])\n",
    "    elif method_name == 'rotation':\n",
    "        augmented_image, augmented_label = augment_rotation(image_path, label_path, **details['parameters'])\n",
    "    elif method_name == 'brightness_contrast':\n",
    "        augmented_image = augment_brightness_contrast(image_path, **details['parameters'])\n",
    "        augmented_label = open(label_path).read()  # No change to label for this augmentation\n",
    "    elif method_name == 'gaussian_noise':\n",
    "        augmented_image = augment_gaussian_noise(image_path, **details['parameters'])\n",
    "        augmented_label = open(label_path).read()  # No change to label for this augmentation\n",
    "    elif method_name == 'histogram_equalization':\n",
    "        augmented_image = augment_histogram_equalization(image_path, **details['parameters'])\n",
    "        augmented_label = open(label_path).read()  # No change to label for this augmentation\n",
    "    elif method_name == 'white_balance':\n",
    "        augmented_image = augment_white_balance(image_path)\n",
    "        augmented_label = open(label_path).read()  # No change to label for this augmentation\n",
    "    elif method_name == 'sharpen':\n",
    "        augmented_image = augment_sharpen(image_path)\n",
    "        augmented_label = open(label_path).read()  # No change to label for this augmentation\n",
    "    elif method_name == 'zoom':\n",
    "        augmented_image, augmented_label = augment_zoom(image_path, label_path, **details['parameters'])\n",
    "    \n",
    "    # Save the augmented image and label\n",
    "    save_image(augmented_dataset_path + '/images/train/' + aug_image_filename, augmented_image)\n",
    "    save_label(augmented_dataset_path + '/labels/train/' + aug_label_filename, augmented_label)\n",
    "\n",
    "\n",
    "def augment_dataset(original_dataset_path, augmentation_metadata):\n",
    "   \n",
    "    # Step 1: Copy the original dataset\n",
    "    augmented_dataset_path = original_dataset_path + '_aug'\n",
    "    if os.path.exists(augmented_dataset_path):\n",
    "        shutil.rmtree(augmented_dataset_path)\n",
    "    #shutil.copytree(original_dataset_path, augmented_dataset_path)\n",
    "    copy_dataset_in_parallel(original_dataset_path, augmented_dataset_path)\n",
    "    print(f\"Copied dataset to: {augmented_dataset_path}\")\n",
    "\n",
    "    # Load the original training images\n",
    "    train_images_dir = original_dataset_path + '/images/train'\n",
    "    train_images = [os.path.join(train_images_dir, img_name) for img_name in os.listdir(train_images_dir)]\n",
    "\n",
    "\n",
    "    # Prepare for augmentations    \n",
    "    total_images = len(train_images)\n",
    "\n",
    "    # Total augmentations for progress bar\n",
    "    total_augmentations = 0\n",
    "    for details in augmentation_metadata['methods'].values():\n",
    "        total_augmentations += int(details['apply_to_percentage'] * total_images)\n",
    "    pbar = tqdm(total=total_augmentations, desc=\"Augmenting Images and Labels\")\n",
    "\n",
    "    # Using ThreadPoolExecutor to parallelize image processing\n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        futures = []\n",
    "        for method_name, details in augmentation_metadata['methods'].items():\n",
    "            num_images_to_augment = int(details['apply_to_percentage'] * total_images)\n",
    "            selected_images = np.random.choice(train_images, num_images_to_augment, replace=False)\n",
    "            \n",
    "            for image_path in selected_images:\n",
    "                futures.append(executor.submit(apply_augmentation, image_path, augmented_dataset_path, method_name, details))\n",
    "        \n",
    "        # Wait for all futures to complete\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Update the progress bar completion and close\n",
    "    pbar.close()\n",
    "\n",
    "    old_dataset_name = os.path.basename(original_dataset_path)\n",
    "\n",
    "    yaml_path = os.path.join(augmented_dataset_path, f'{old_dataset_name}.yaml')\n",
    "    with open(yaml_path, 'r') as file:\n",
    "        yaml_data = yaml.safe_load(file)\n",
    "\n",
    "    # Append '-augmented' to the dataset path in the YAML data\n",
    "    yaml_data['path'] = yaml_data['path'] + '_aug'\n",
    "\n",
    "    # Save the updated YAML data to a new file with '-augmented' appended to the filename\n",
    "    augmented_yaml_path = yaml_path.replace('.yaml', '_aug.yaml')\n",
    "    with open(augmented_yaml_path, 'w') as file:\n",
    "        yaml.dump(yaml_data, file, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "    os.remove(yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6c0b9404a642ea932f3a3c77da0fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Copying dataset:   0%|          | 0/48001 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied dataset to: datasets/cessna_only/dataset_aug\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5baecb8dedf4aa4b32d8b75a3b76f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Augmenting Images and Labels:   0%|          | 0/2160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset_dir = \"datasets/cessna_only/dataset\"\n",
    "\n",
    "#augmentation_metadata = {\n",
    "#    'methods': {\n",
    "#        'flip': {\n",
    "#            'parameters': {\n",
    "#                'orientation': 'h',  # Could be 'h' for horizontal or 'v' for vertical\n",
    "#                'p': 1.0  # Probability of applying the augmentation\n",
    "#            },\n",
    "#            'apply_to_percentage': 1  # 50% of the training images\n",
    "#        },\n",
    "#        'rotation': {\n",
    "#            'parameters': {\n",
    "#                'angle': 45,  # Rotation angle\n",
    "#                'p': 1.0  # Probability of applying the augmentation\n",
    "#            },\n",
    "#            'apply_to_percentage': 1  # 30% of the training images\n",
    "#        },\n",
    "#        'brightness_contrast': {\n",
    "#            'parameters': {\n",
    "#                'alpha': 1.5,  # Contrast control (1.0-3.0)\n",
    "#                'beta': 50     # Brightness control (-100 to 100)\n",
    "#            },\n",
    "#            'apply_to_percentage': 0.2  # 20% of the training images\n",
    "#        },\n",
    "#        'gaussian_noise': {\n",
    "#            'parameters': {\n",
    "#                'var_limit': (10.0, 50.0),  # Variance range for noise\n",
    "#            },\n",
    "#            'apply_to_percentage': 1  # 10% of the training images\n",
    "#        },\n",
    "#        # Add other methods as needed\n",
    "#    }\n",
    "#}\n",
    "\n",
    "augmentation_metadata = {\n",
    "    'methods': {\n",
    "        'flip': {\n",
    "            'parameters': {\n",
    "                'orientation': 'h',  # Could be 'h' for horizontal or 'v' for vertical\n",
    "                'p': 1.0  # Probability of applying the augmentation\n",
    "            },\n",
    "            'apply_to_percentage': 0.1  # 50% of the training images\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "augment_dataset(test_dataset_dir, augmentation_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for each specfic test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Own Implementation of Object Detection System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
