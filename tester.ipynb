{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from ultralytics import settings\n",
    "from ultralytics import YOLO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'settings_version': '0.0.4', 'datasets_dir': 'D:\\\\Tom\\\\GitHub\\\\Third Year Project\\\\datasets', 'weights_dir': 'D:\\\\Tom\\\\GitHub\\\\Third Year Project\\\\Intruder-Aircraft-Detection\\\\weights', 'runs_dir': 'D:\\\\Tom\\\\GitHub\\\\Third Year Project\\\\Intruder-Aircraft-Detection\\\\runs', 'uuid': 'fdb5c10788ffaa41a9047dc764dd8a0a3287d6bec12c0c66234ac745c0366efa', 'sync': True, 'api_key': '', 'openai_api_key': '', 'clearml': True, 'comet': True, 'dvc': True, 'hub': True, 'mlflow': True, 'neptune': True, 'raytune': True, 'tensorboard': True, 'wandb': True}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Ultralytics settings\n",
    "print(settings) \n",
    "\n",
    "# GPU Utilisation\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64800 entries, 0 to 64799\n",
      "Data columns (total 23 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   imageID      64800 non-null  int64  \n",
      " 1   image_path   64800 non-null  object \n",
      " 2   label_path   64800 non-null  object \n",
      " 3   location     64800 non-null  object \n",
      " 4   enrange      64800 non-null  float64\n",
      " 5   urange       64800 non-null  float64\n",
      " 6   weather      64800 non-null  int64  \n",
      " 7   daystart     64800 non-null  float64\n",
      " 8   dayend       64800 non-null  float64\n",
      " 9   num_train    64800 non-null  int64  \n",
      " 10  num_valid    64800 non-null  int64  \n",
      " 11  append       64800 non-null  bool   \n",
      " 12  datasetname  64800 non-null  object \n",
      " 13  ac           64800 non-null  object \n",
      " 14  allweather   64800 non-null  bool   \n",
      " 15  newac        64800 non-null  bool   \n",
      " 16  own_h        64800 non-null  object \n",
      " 17  own_p_max    64800 non-null  float64\n",
      " 18  own_r_max    64800 non-null  float64\n",
      " 19  intr_h       64800 non-null  object \n",
      " 20  vfov         64800 non-null  float64\n",
      " 21  hfov         64800 non-null  float64\n",
      " 22  outdir       64800 non-null  object \n",
      "dtypes: bool(3), float64(8), int64(4), object(8)\n",
      "memory usage: 10.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Base paths for the images and labels\n",
    "train_images_path = 'datasets/AVOIDDS/images/train'\n",
    "train_labels_path = 'datasets/AVOIDDS/labels/train'\n",
    "val_images_path = 'datasets/AVOIDDS/images/valid'\n",
    "val_labels_path = 'datasets/AVOIDDS/labels/valid'\n",
    "\n",
    "# Load the metadata\n",
    "metadata_path = 'datasets/AVOIDDS/metadata.json'\n",
    "with open(metadata_path, 'r') as file:\n",
    "    metadata = json.load(file)\n",
    "\n",
    "# Function to create a DataFrame from images and labels\n",
    "def create_dataframe(images_path, labels_path, metadata):\n",
    "\n",
    "    # List all files in the directories\n",
    "    image_files = [f for f in sorted(os.listdir(images_path)) if f.endswith('.jpg')]\n",
    "    label_files = [f for f in sorted(os.listdir(labels_path)) if f.endswith('.txt')]\n",
    "    \n",
    "    # Create tempory DataFrame so that final dataframe is in correct order\n",
    "    temp_df = pd.DataFrame({\n",
    "        'image_path': [str(images_path + '/' + file) for file in image_files],\n",
    "        'label_path': [str(labels_path + '/' + file) for file in label_files],\n",
    "    })\n",
    "\n",
    "    # Extract image indices to match with metadata\n",
    "    df = pd.DataFrame()\n",
    "    df['imageID'] = temp_df['image_path'].apply(lambda x: int(os.path.splitext(os.path.basename(x))[0]))\n",
    "\n",
    "    # Add image and label paths to final dataframe\n",
    "    df['image_path'] = temp_df['image_path']\n",
    "    df['label_path'] = temp_df['label_path']\n",
    " \n",
    "    # Add metadata to each image entry\n",
    "    for key, value in metadata.items():\n",
    "        if '.' in key:  # Key represents a range\n",
    "            start, end = map(int, key.split('.'))\n",
    "            df.loc[df['imageID'].between(start, end), 'metadata'] = json.dumps(value)\n",
    "\n",
    "    # Convert the JSON strings in 'metadata' to dictionaries\n",
    "    df['metadata'] = df['metadata'].apply(json.loads)\n",
    "\n",
    "    # Expand the 'metadata' column into separate columns\n",
    "    metadata_df = pd.json_normalize(df['metadata'])\n",
    "    \n",
    "    # Concatenate the expanded metadata back to the original DataFrame\n",
    "    full_df = pd.concat([df.drop(['metadata'], axis=1), metadata_df], axis=1)\n",
    "\n",
    "    return full_df\n",
    "\n",
    "# Create the DataFrames for the train and validation sets\n",
    "train_df = create_dataframe(train_images_path, train_labels_path, metadata)\n",
    "valid_df = create_dataframe(val_images_path, val_labels_path, metadata)\n",
    "\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create sub datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataset(dataset_name, filtered_train_df, filtered_valid_df, class_names=['aircraft'], dataset_dir=\"datasets/\"):\n",
    "    # Construct the base path using the dataset name\n",
    "    dataset_dir = Path(dataset_dir) / dataset_name\n",
    "    \n",
    "    # Define subdirectories for images and labels\n",
    "    images_dir = dataset_dir / 'images'\n",
    "    labels_dir = dataset_dir / 'labels'\n",
    "    \n",
    "    # Create the necessary directories\n",
    "    for subdir in ['train', 'valid']:\n",
    "        (images_dir / subdir).mkdir(parents=True, exist_ok=True)\n",
    "        (labels_dir / subdir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"created directories\")\n",
    "\n",
    "    # Function to copy files from the DataFrame to the respective directories\n",
    "    #def copy_files(df, img_dest_dir, label_dest_dir):\n",
    "    #    def copy_file(row):\n",
    "    #        shutil.copy(row['image_path'], img_dest_dir / f\"{Path(row['image_path']).name}\")\n",
    "    #        shutil.copy(row['label_path'], label_dest_dir / f\"{Path(row['label_path']).name}\")\n",
    "    #        \n",
    "    #    # Wrap iterrows with tqdm for a progress bar\n",
    "    #    list_of_rows = list(df.iterrows())\n",
    "    #    for _ in tqdm(range(len(list_of_rows)), desc='Copying files'):\n",
    "    #        # Use ThreadPoolExecutor to copy files in parallel\n",
    "    #        with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    #            executor.map(copy_file, [row for _, row in list_of_rows])\n",
    "    \n",
    "    def copy_files(df, img_dest_dir, label_dest_dir):\n",
    "        # Initialize tqdm progress bar with the total length of the DataFrame\n",
    "        pbar = tqdm(total=len(df), desc='Copying files')\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Copy files\n",
    "            shutil.copy(row['image_path'], img_dest_dir / f\"{Path(row['image_path']).name}\")\n",
    "            shutil.copy(row['label_path'], label_dest_dir / f\"{Path(row['label_path']).name}\")\n",
    "            # Update the progress bar\n",
    "            pbar.update(1)\n",
    "        # Close the progress bar after completion\n",
    "        pbar.close()\n",
    "\n",
    "    # Copy the files for the train and validation sets\n",
    "    copy_files(filtered_train_df, images_dir / 'train', labels_dir / 'train')\n",
    "    print(\"copied train files\")\n",
    "    copy_files(filtered_valid_df, images_dir / 'valid', labels_dir / 'valid')\n",
    "    print(\"copied valid files\")\n",
    "    \n",
    "    # Create the .yaml configuration file for the dataset\n",
    "    yaml_content = {\n",
    "        'path': str(dataset_dir),  # Optionally include the path in the YAML\n",
    "        'train': str(images_dir / 'train'),\n",
    "        'val': str(images_dir / 'valid'),\n",
    "        #'nc': len(class_names),\n",
    "        'names': class_names\n",
    "    }\n",
    "    \n",
    "    # Create yaml file\n",
    "    yaml_path = dataset_dir / f\"{dataset_name}.yaml\"\n",
    "    with open(yaml_path, 'w') as file:\n",
    "        yaml.dump(yaml_content, file, sort_keys=False)\n",
    "    \n",
    "    print(f\"Dataset '{dataset_name}' created at {dataset_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cessna_train_df = train_df[train_df['ac'] == 'Cessna Skyhawk']\n",
    "cessna_valid_df = valid_df[valid_df['ac'] == 'Cessna Skyhawk']\n",
    "\n",
    "dataset_name = 'cessna_dataset'\n",
    "\n",
    "create_dataset(dataset_name, cessna_train_df, cessna_valid_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label(label_path):\n",
    "\n",
    "    # Placeholder for reading the label file in YOLO format\n",
    "    pass\n",
    "\n",
    "def write_label(label_path, bboxes, image_shape):\n",
    "    \n",
    "    # Placeholder for writing the label file in YOLO format\n",
    "    pass\n",
    "\n",
    "def augment_image(image_path, label_path, augmentation):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    bboxes = read_label(label_path)\n",
    "\n",
    "    # Note that bboxes should be normalized (i.e., in the range [0, 1])\n",
    "    transformed = augmentation(image=image, bboxes=bboxes)\n",
    "    transformed_image = transformed['image']\n",
    "    transformed_bboxes = transformed['bboxes']\n",
    "\n",
    "    # Denormalize bboxes here if your write_label function expects that\n",
    "    write_label(label_path, transformed_bboxes, transformed_image.shape)\n",
    "\n",
    "    cv2.imwrite(image_path, cv2.cvtColor(transformed_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "# Zoom in/out\n",
    "def zoom(image_path, label_path, min_zoom=0.8, max_zoom=1.2):\n",
    "    augmentation = A.Compose([\n",
    "        A.RandomScale(scale_limit=(min_zoom - 1, max_zoom - 1), p=1.0)\n",
    "    ], bbox_params=A.BboxParams(format='yolo', label_fields=[]))\n",
    "    \n",
    "    augment_image(image_path, label_path, augmentation)\n",
    "\n",
    "\n",
    "# Horizontal and Vertical Reflection\n",
    "def reflection(image_path, label_path):\n",
    "    augmentation = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5)\n",
    "    ], bbox_params=A.BboxParams(format='yolo', label_fields=[]))\n",
    "    \n",
    "    augment_image(image_path, label_path, augmentation)\n",
    "\n",
    "# Rotation\n",
    "def rotate(image_path, label_path, angle_limit=90):\n",
    "    augmentation = A.Compose([\n",
    "        A.Rotate(limit=angle_limit, p=1.0)\n",
    "    ], bbox_params=A.BboxParams(format='yolo', label_fields=[]))\n",
    "    \n",
    "    augment_image(image_path, label_path, augmentation)\n",
    "\n",
    "# Contrast and Brightness\n",
    "def adjust_contrast_brightness(image_path, label_path, brightness_limit=0.2, contrast_limit=0.2):\n",
    "    augmentation = A.Compose([\n",
    "        A.RandomBrightnessContrast(brightness_limit=brightness_limit, contrast_limit=contrast_limit, p=1.0)\n",
    "    ])\n",
    "    \n",
    "    augment_image(image_path, label_path, augmentation)\n",
    "\n",
    "# Histogram Equalization\n",
    "def histogram_equalization(image_path, label_path):\n",
    "    augmentation = A.Compose([\n",
    "        A.HistogramMatching(p=1.0)\n",
    "    ])\n",
    "    \n",
    "    augment_image(image_path, label_path, augmentation)\n",
    "\n",
    "# White Balance (using FancyPCA for color augmentation which can simulate white balance shifts)\n",
    "def white_balance(image_path, label_path):\n",
    "    augmentation = A.Compose([\n",
    "        A.FancyPCA(alpha=0.1, p=1.0)\n",
    "    ])\n",
    "    \n",
    "    augment_image(image_path, label_path, augmentation)\n",
    "\n",
    "# Sharpen\n",
    "def sharpen(image_path, label_path):\n",
    "    augmentation = A.Compose([\n",
    "        A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=1.0)\n",
    "    ])\n",
    "    \n",
    "    augment_image(image_path, label_path, augmentation)\n",
    "\n",
    "# Gaussian Noise\n",
    "def gaussian_noise(image_path, label_path, var_limit=(10.0, 50.0)):\n",
    "    augmentation = A.Compose([\n",
    "        A.GaussNoise(var_limit=var_limit, p=1.0)\n",
    "    ])\n",
    "    \n",
    "    augment_image(image_path, label_path, augmentation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for each specfic test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Own Implementation of Object Detection System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
