{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset as ds\n",
    "import config as cfg\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Rescaling, Dropout\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "from clearml import Task\n",
    "import clearml\n",
    "clearml.browser_login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClearML\n",
    "https://app.clear.ml/dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base paths for the images and labels\n",
    "train_images_path = f'{cfg.BASE_DATASET}/images/train'\n",
    "train_labels_path = f'{cfg.BASE_DATASET}/labels/train'\n",
    "val_images_path = f'{cfg.BASE_DATASET}/images/valid'\n",
    "val_labels_path = f'{cfg.BASE_DATASET}/labels/valid'\n",
    "\n",
    "# Base path for metadata\n",
    "metadata_path = f'{cfg.BASE_DATASET}/metadata.json'\n",
    "\n",
    "# Create the DataFrames for the train and validation sets\n",
    "train_df = ds.create_dataframe(train_images_path, train_labels_path, metadata_path)\n",
    "valid_df = ds.create_dataframe(val_images_path, val_labels_path, metadata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name= cfg.PROJECT_NAME + '/tf_clf'\n",
    "dataset_name = 'test500'\n",
    "dataset_dir = cfg.CLF_DATASET_DIR + f'/{dataset_name}'\n",
    "project_dir = f'{cfg.CLF_PROJECT_DIR}/{dataset_name}/'\n",
    "class_names = cfg.CLF_CLASS_NAMES\n",
    "\n",
    "epochs = 20\n",
    "zoom_factor = 1.5\n",
    "\n",
    "\n",
    "_, test_train_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=500,  # Number of items you want in your sample\n",
    "    stratify=train_df['ac'],  # Stratify based on the combined column\n",
    "    #random_state=42  # Ensures reproducibility\n",
    ")\n",
    "\n",
    "_, test_val_df = train_test_split(\n",
    "    valid_df,\n",
    "    test_size=100,  # Number of items you want in your sample\n",
    "    stratify=valid_df['ac'],  # Stratify based on the combined column\n",
    "    #random_state=42  # Ensures reproducibility\n",
    ")\n",
    "\n",
    "ds.create_sub_dataset(dataset_dir, test_train_df, test_val_df, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing to AID classification\n",
    "\n",
    "ds.pre_process_dataset_for_classification(dataset_dir, zoom_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.correct_dataset_labels(dataset_dir, test_train_df, test_val_df, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_metadata = {\n",
    "    'methods': {        \n",
    "        'flip': {\n",
    "            'parameters': {\n",
    "                'orientation': 'h',  # Could be 'h' for horizontal or 'v' for vertical\n",
    "                'p': 1.0  # Probability of applying the augmentation\n",
    "            },\n",
    "            'apply_to_percentage': 0.5  # 50% of the training images\n",
    "        }        \n",
    "    }\n",
    "}\n",
    "\n",
    "ds.augment_dataset(dataset_dir, augmentation_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.reorganize_dataset_for_keras(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classifier Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(dataset_dir,'images','train')\n",
    "train_aug_dir = os.path.join(dataset_dir,'images','train-aug')\n",
    "valid_dir = os.path.join(dataset_dir,'images','valid')\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "# load datasets using keras\n",
    "train_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True)\n",
    "\n",
    "train_aug_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_aug_dir,\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True)\n",
    "\n",
    "valid_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    valid_dir,\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    "    image_size=(img_height, img_width),    \n",
    "    shuffle=True)\n",
    "\n",
    "class_names = train_data.class_names\n",
    "print(class_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test visualise \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_aug_data.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_enc(image, label):\n",
    "    return image, tf.one_hot(label, len(class_names))\n",
    "\n",
    "train_data = train_data.map(one_hot_enc)\n",
    "train_aug_data = train_aug_data.map(one_hot_enc)\n",
    "valid_data = valid_data.map(one_hot_enc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_data:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_data = train_data.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "train_aug_data = train_aug_data.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "def get_tf_model(): \n",
    "  num_classes = len(class_names)\n",
    "\n",
    "  model = Sequential([\n",
    "    Input(shape=(img_height, img_width, 3)),\n",
    "    Rescaling(1./255),\n",
    "    Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "   #Conv2D(32, (3,3), activation='relu'),\n",
    "   #MaxPooling2D(),\n",
    "   #Conv2D(32, (3,3), activation='relu'),\n",
    "   #MaxPooling2D(),\n",
    "   #Conv2D(16, (3,3), activation='relu'),\n",
    "   #MaxPooling2D(),\n",
    "   #Dropout(0.2),\n",
    "   #Flatten(),\n",
    "   #Dense(256, activation='relu'),      \n",
    "    Dense(num_classes, activation='softmax')\n",
    " \n",
    "  ])\n",
    "  \n",
    "  \"\"\"model = Sequential()\n",
    "    model.add(Input(shape=(256,256,3)))  # Add an Input layer to specify the input shape\n",
    "    model.add(Conv2D(32, (3,3), activation='relu')) \n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(16, (3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))  # Assuming you have 3 classes \"\"\"\n",
    "\n",
    "  model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n",
    "\n",
    "  #model.summary()\n",
    "  \n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train pure\n",
    "\n",
    "# local logs directory\n",
    "logs_dir=cfg.CLF_PROJECT_DIR\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "\n",
    "#connect to clearml\n",
    "task = Task.init(project_name=project_name, task_name=f\"{dataset_name}-pure-eps:{epochs}-zf:{zoom_factor}\")\n",
    "logger = task.get_logger()\n",
    "\n",
    "# clearml hyperparameters\n",
    "hyper_params = {'epochs': epochs, 'zoom_factor': zoom_factor, 'batch_size': batch_size, 'img_height': img_height, 'img_width': img_width, 'class_names': class_names}\n",
    "task.connect(hyper_params)\n",
    "\n",
    "model = None\n",
    "train_hst = None\n",
    "\n",
    "model = get_tf_model()\n",
    "\n",
    "# trian model\n",
    "train_hst = model.fit(\n",
    "    train_data, \n",
    "    epochs=hyper_params['epochs'], \n",
    "    validation_data=valid_data,\n",
    "    callbacks=[tensorboard_callback])\n",
    "\n",
    "# send metrics to clearML\n",
    "for epoch in range(epochs):\n",
    "    # Log training metrics\n",
    "    logger.report_scalar('loss', 'train', iteration=epoch, value=train_hst.history['loss'][epoch])\n",
    "    logger.report_scalar('accuracy', 'train', iteration=epoch, value=train_hst.history['accuracy'][epoch])\n",
    "    logger.report_scalar('precision', 'train', iteration=epoch, value=train_hst.history['precision'][epoch])\n",
    "    logger.report_scalar('recall', 'train', iteration=epoch, value=train_hst.history['recall'][epoch])\n",
    "\n",
    "    # Log validation metrics\n",
    "    logger.report_scalar('loss', 'validation', iteration=epoch, value=train_hst.history['val_loss'][epoch])\n",
    "    logger.report_scalar('accuracy', 'validation', iteration=epoch, value=train_hst.history['val_accuracy'][epoch])\n",
    "    logger.report_scalar('precision', 'validation', iteration=epoch, value=train_hst.history['val_precision'][epoch])\n",
    "    logger.report_scalar('recall', 'validation', iteration=epoch, value=train_hst.history['val_recall'][epoch])\n",
    "\n",
    "# close task\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train aug\n",
    "\n",
    "# local logs directory\n",
    "logs_dir=cfg.CLF_PROJECT_DIR\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "\n",
    "#connect to clearml\n",
    "task = Task.init(project_name=project_name, task_name=f\"{dataset_name}-aug-eps:{epochs}-zf:{zoom_factor}\")\n",
    "logger = task.get_logger()\n",
    "\n",
    "\n",
    "# clearml hyperparameters\n",
    "hyper_params = {'epochs': epochs, 'zoom_factor': zoom_factor, 'batch_size': batch_size, 'img_height': img_height, 'img_width': img_width, 'class_names': class_names}\n",
    "task.connect(hyper_params)\n",
    "\n",
    "model_aug = None\n",
    "train_aug_hst = None\n",
    "\n",
    "# reset model from previous test\n",
    "model_aug = get_tf_model()\n",
    "\n",
    "# trian model\n",
    "train_aug_hst = model_aug.fit(\n",
    "    train_aug_data, \n",
    "    epochs=hyper_params['epochs'], \n",
    "    validation_data=valid_data, \n",
    "    callbacks=[tensorboard_callback])\n",
    "\n",
    "# send metrics to clearML\n",
    "for epoch in range(epochs):\n",
    "    # Log training metrics\n",
    "    logger.report_scalar('loss', 'train', iteration=epoch, value=train_aug_hst.history['loss'][epoch])\n",
    "    logger.report_scalar('accuracy', 'train', iteration=epoch, value=train_aug_hst.history['accuracy'][epoch])\n",
    "    logger.report_scalar('precision', 'train', iteration=epoch, value=train_aug_hst.history['precision'][epoch])\n",
    "    logger.report_scalar('recall', 'train', iteration=epoch, value=train_aug_hst.history['recall'][epoch])\n",
    "\n",
    "    # Log validation metrics\n",
    "    logger.report_scalar('loss', 'validation', iteration=epoch, value=train_aug_hst.history['val_loss'][epoch])\n",
    "    logger.report_scalar('accuracy', 'validation', iteration=epoch, value=train_aug_hst.history['val_accuracy'][epoch])\n",
    "    logger.report_scalar('precision', 'validation', iteration=epoch, value=train_aug_hst.history['val_precision'][epoch])\n",
    "    logger.report_scalar('recall', 'validation', iteration=epoch, value=train_aug_hst.history['val_recall'][epoch])\n",
    "    \n",
    "# close task\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classifier (redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check GPU available\n",
    "#gpus = tf.config.list_physical_devices('GPU')\n",
    "#print(gpus)\n",
    "#\n",
    "## limit vram usage\n",
    "#for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories of train, train-aug and validation\n",
    "#train_dir = os.path.join(dataset_dir,'images','train')\n",
    "#train_aug_dir = os.path.join(dataset_dir,'images','train-aug')\n",
    "#valid_dir = os.path.join(dataset_dir,'images','valid')\n",
    "#\n",
    "## load datasets using keras\n",
    "#train_data = tf.keras.utils.image_dataset_from_directory(train_dir,\n",
    "#                                                         batch_size=32,\n",
    "#                                                        seed=42, \n",
    "#                                                         shuffle=True)\n",
    "#train_aug_data = tf.keras.utils.image_dataset_from_directory(train_aug_dir,\n",
    "#                                                             batch_size=32,\n",
    "#                                                             seed=42,\n",
    "#                                                             shuffle=True)\n",
    "#valid_data = tf.keras.utils.image_dataset_from_directory(valid_dir,\n",
    "#                                                         batch_size=32,\n",
    "#                                                         seed=42,\n",
    "#                                                         shuffle=True)\n",
    "#\n",
    "## scale images between 0 and 1\n",
    "#train_data = train_data.map(lambda x,y: (x/255, y))\n",
    "#train_aug_data = train_aug_data.map(lambda x,y: (x/255, y))\n",
    "#valid_data = valid_data.map(lambda x,y: (x/255, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - use numpy to turn dataset into iterator for ease of use with batches, call each batch with .next()\n",
    "#train_iterator = train_data.as_numpy_iterator()\n",
    "#train_batch = train_iterator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST - visualise images with class labels\n",
    "#train_batch = train_iterator.next()\n",
    "#fig, ax = plt.subplots(ncols=4, figsize=(20, 20))\n",
    "#for idx, img in enumerate(train_batch[0][:4]):\n",
    "#    ax[idx].imshow(img)\n",
    "#    ax[idx].title.set_text(train_batch[1][idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use numpy to turn dataset into iterator for ease of use with batches, call each batch with .next()\n",
    "#train_data.as_numpy_iterator().next()\n",
    "#train_aug_data.as_numpy_iterator().next()\n",
    "#valid_data.as_numpy_iterator().next()\n",
    "#\n",
    "#print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a CNN model\n",
    "#def get_model():\n",
    "#\n",
    "#    tf.random.set_seed(42)\n",
    "#    # model architecture - Also try ResNet50 or VGG16\n",
    "#    model = Sequential()\n",
    "#    model.add(Input(shape=(256,256,3)))  # Add an Input layer to specify the input shape\n",
    "#    model.add(Conv2D(32, (3,3), activation='relu')) \n",
    "#    model.add(MaxPooling2D())\n",
    "#    model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "#    model.add(MaxPooling2D())\n",
    "#    model.add(Conv2D(16, (3,3), activation='relu'))\n",
    "#    model.add(MaxPooling2D())\n",
    "#    model.add(Flatten())\n",
    "#    model.add(Dense(256, activation='relu'))\n",
    "#    model.add(Dense(3, activation='softmax'))  # Assuming you have 3 classes\n",
    "#\n",
    "#    model.compile(optimizer='adam', \n",
    "#                loss='sparse_categorical_crossentropy', \n",
    "#                metrics=['accuracy'])\n",
    "#    #model.summary()\n",
    "#    \n",
    "#    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train pure\n",
    "\n",
    "# local logs directory\n",
    "#logs_dir=cfg.CLF_PROJECT_DIR\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "#\n",
    "##connect to clearml\n",
    "#task = Task.init(project_name=project_name, task_name=f\"{dataset_name}-pure-eps:{epochs}-zf:{zoom_factor}\")\n",
    "#logger = task.get_logger()\n",
    "#\n",
    "## clearml hyperparameters\n",
    "#hyper_params = {'epochs': epochs}\n",
    "#task.connect(hyper_params)\n",
    "#\n",
    "#model = get_model()\n",
    "#\n",
    "## trian model\n",
    "#train_hst = model.fit(\n",
    "#    train_data, \n",
    "#    epochs=hyper_params['epochs'], \n",
    "#    validation_data=valid_data, \n",
    "#    callbacks=[tensorboard_callback])\n",
    "#\n",
    "## send metrics to clearML\n",
    "#for epoch in range(epochs):\n",
    "#    # Log training metrics\n",
    "#    logger.report_scalar('loss', 'train', iteration=epoch, value=train_hst.history['loss'][epoch])\n",
    "#    logger.report_scalar('accuracy', 'train', iteration=epoch, value=train_hst.history['accuracy'][epoch])\n",
    "#\n",
    "#    # Log validation metrics\n",
    "#    logger.report_scalar('loss', 'validation', iteration=epoch, value=train_hst.history['val_loss'][epoch])\n",
    "#    logger.report_scalar('accuracy', 'validation', iteration=epoch, value=train_hst.history['val_accuracy'][epoch])\n",
    "#\n",
    "## close task\n",
    "#task.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train aug\n",
    "\n",
    "# local logs directory\n",
    "#logs_dir=cfg.CLF_PROJECT_DIR\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "#\n",
    "##connect to clearml\n",
    "#task = Task.init(project_name=project_name, task_name=f\"{dataset_name}-aug-eps:{epochs}-zf:{zoom_factor}\")\n",
    "#logger = task.get_logger()\n",
    "#\n",
    "#\n",
    "## clearml hyperparameters\n",
    "#hyper_params = {'epochs': epochs}\n",
    "#task.connect(hyper_params)\n",
    "#\n",
    "## reset model from previous test\n",
    "#model_aug = get_model()\n",
    "#\n",
    "## trian model\n",
    "#train_aug_hst = model.fit(\n",
    "#    train_aug_data, \n",
    "#    epochs=hyper_params['epochs'], \n",
    "#    validation_data=valid_data, \n",
    "#    callbacks=[tensorboard_callback])\n",
    "#\n",
    "## send metrics to clearML\n",
    "#for epoch in range(epochs):\n",
    "#    # Log training metrics\n",
    "#    logger.report_scalar('loss', 'train', iteration=epoch, value=train_aug_hst.history['loss'][epoch])\n",
    "#    logger.report_scalar('accuracy', 'train', iteration=epoch, value=train_aug_hst.history['accuracy'][epoch])\n",
    "#\n",
    "#    # Log validation metrics\n",
    "#    logger.report_scalar('loss', 'validation', iteration=epoch, value=train_aug_hst.history['val_loss'][epoch])\n",
    "#    logger.report_scalar('accuracy', 'validation', iteration=epoch, value=train_aug_hst.history['val_accuracy'][epoch])\n",
    "#    \n",
    "## close task\n",
    "#task.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
