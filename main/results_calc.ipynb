{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 91\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     54\u001b[0m metric_tables \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"                              Metric  Training  Validation\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m0                   Average Accuracy     0.760       0.655\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;66;03m# More tables would follow here\u001b[39;00m\n\u001b[0;32m     89\u001b[0m ]\n\u001b[1;32m---> 91\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_metric_strings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetric_tables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m, in \u001b[0;36mprocess_metric_strings\u001b[1;34m(list_of_metric_strings)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Concatenate data from each string into a single DataFrame\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric_string \u001b[38;5;129;01min\u001b[39;00m list_of_metric_strings:\n\u001b[1;32m---> 26\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mstring_to_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetric_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     aggregated_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([aggregated_data, df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Group by 'Metric' and calculate the mean and std for each metric\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m, in \u001b[0;36mprocess_metric_strings.<locals>.string_to_df\u001b[1;34m(metric_string)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parts) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m     16\u001b[0m         index, metric, values \u001b[38;5;241m=\u001b[39m parts\n\u001b[1;32m---> 17\u001b[0m         training, validation \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     18\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend([metric, training, validation])\n\u001b[0;32m     19\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetric\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_metric_strings(list_of_metric_strings):\n",
    "    # Initialize an empty DataFrame to store all the data\n",
    "    aggregated_data = pd.DataFrame()\n",
    "\n",
    "    # Function to process each metric string into a DataFrame\n",
    "    def string_to_df(metric_string):\n",
    "        # Split the string by lines and process into a DataFrame\n",
    "        lines = metric_string.strip().split(\"\\n\")\n",
    "        data = []\n",
    "        for line in lines[1:]:  # Skip the header line\n",
    "            parts = line.split(maxsplit=2)\n",
    "            if len(parts) == 3:\n",
    "                index, metric, values = parts\n",
    "                training, validation = values.split()\n",
    "                data.append([metric, training, validation])\n",
    "        df = pd.DataFrame(data, columns=['Metric', 'Training', 'Validation'])\n",
    "        df['Training'] = pd.to_numeric(df['Training'], errors='coerce')\n",
    "        df['Validation'] = pd.to_numeric(df['Validation'], errors='coerce')\n",
    "        return df\n",
    "\n",
    "    # Concatenate data from each string into a single DataFrame\n",
    "    for metric_string in list_of_metric_strings:\n",
    "        df = string_to_df(metric_string)\n",
    "        aggregated_data = pd.concat([aggregated_data, df], ignore_index=True)\n",
    "\n",
    "    # Group by 'Metric' and calculate the mean and std for each metric\n",
    "    summary = aggregated_data.groupby('Metric').agg({\n",
    "        'Training': [('Training Mean', 'mean'), ('Training Std', 'std')],\n",
    "        'Validation': [('Validation Mean', 'mean'), ('Validation Std', 'std')]\n",
    "    })\n",
    "\n",
    "    # Flatten the multi-level columns and handle missing values\n",
    "    summary.columns = [' '.join(col).strip() for col in summary.columns.values]\n",
    "    summary.reset_index(inplace=True)\n",
    "    \n",
    "    # Replace NaN in tuples with the original 'NaN' and format the results\n",
    "    formatted_results = []\n",
    "    for _, row in summary.iterrows():\n",
    "        formatted_results.append({\n",
    "            'Metric': row['Metric'],\n",
    "            'Training': (f\"{row['Training Mean']:.3f}\", f\"{row['Training Std']:.3f}\") if pd.notna(row['Training Mean']) else ('NaN', 'NaN'),\n",
    "            'Validation': (f\"{row['Validation Mean']:.3f}\", f\"{row['Validation Std']:.3f}\") if pd.notna(row['Validation Mean']) else ('NaN', 'NaN')\n",
    "        })\n",
    "\n",
    "    result_df = pd.DataFrame(formatted_results)\n",
    "    \n",
    "    # Convert DataFrame to string maintaining the table format\n",
    "    return result_df.to_string(index=False)\n",
    "\n",
    "# Example usage\n",
    "metric_tables = [\n",
    "    \"\"\"                              Metric  Training  Validation\n",
    "0                   Average Accuracy     0.760       0.655\n",
    "1     Standard Deviation of Accuracy     0.223       0.146\n",
    "2                      Best Accuracy     0.959       0.773\n",
    "3                      Last Accuracy     0.956       0.773\n",
    "4                        Overall AUC       NaN       0.489\n",
    "5                   Maximum F1 Score       NaN       0.770\n",
    "6                       Minimum Loss       NaN       0.757\n",
    "7  Difference in Average Loss Last N       NaN       0.846\n",
    "8  Standard Deviation of Loss Last N       NaN       0.119\"\"\",\n",
    "\n",
    "\"\"\"                              Metric  Training  Validation\n",
    "0                   Average Accuracy     0.629       0.552\n",
    "1     Standard Deviation of Accuracy     0.227       0.154\n",
    "2                      Best Accuracy     0.907       0.714\n",
    "3                      Last Accuracy     0.906       0.709\n",
    "4                        Overall AUC       NaN       0.512\n",
    "5                   Maximum F1 Score       NaN       0.708\n",
    "6                       Minimum Loss       NaN       0.796\n",
    "7  Difference in Average Loss Last N       NaN       0.735\n",
    "8  Standard Deviation of Loss Last N       NaN       0.212\"\"\",\n",
    "\n",
    "\"\"\"                              Metric  Training  Validation\n",
    "0                   Average Accuracy     0.663       0.601\n",
    "1     Standard Deviation of Accuracy     0.227       0.163\n",
    "2                      Best Accuracy     0.925       0.783\n",
    "3                      Last Accuracy     0.897       0.697\n",
    "4                        Overall AUC       NaN       0.597\n",
    "5                   Maximum F1 Score       NaN       0.778\n",
    "6                       Minimum Loss       NaN       0.856\n",
    "7  Difference in Average Loss Last N       NaN       0.711\n",
    "8  Standard Deviation of Loss Last N       NaN       0.125\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # More tables would follow here\n",
    "]\n",
    "\n",
    "result = process_metric_strings(metric_tables)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
