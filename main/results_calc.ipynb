{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Metric       Training     Validation\n",
      "                 Average Accuracy (0.499, 0.215) (0.465, 0.164)\n",
      "   Standard Deviation of Accuracy (0.091, 0.111) (0.078, 0.081)\n",
      "                    Best Accuracy (0.605, 0.321) (0.562, 0.214)\n",
      "                    Last Accuracy (0.597, 0.325) (0.538, 0.235)\n",
      "                      Overall AUC            NaN (0.505, 0.031)\n",
      "                 Maximum F1 Score            NaN (0.319, 0.433)\n",
      "                     Minimum Loss            NaN (0.948, 0.199)\n",
      "Difference in Average Loss Last N            NaN (0.320, 0.520)\n",
      "Standard Deviation of Loss Last N            NaN (0.058, 0.079)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def process_metric_tables(metric_tables):\n",
    "    def parse_tables_manual(metric_tables):\n",
    "        dataframes = []\n",
    "        for table in metric_tables:\n",
    "            lines = table.strip().split('\\n')\n",
    "            headers = lines[0].split(maxsplit=2)\n",
    "            data = []\n",
    "            for line in lines[1:]:\n",
    "                index_space = line.index(' ')\n",
    "                first_split = line[:index_space].strip()\n",
    "                remaining = line[index_space:].strip()\n",
    "                parts = remaining.rsplit(maxsplit=2)\n",
    "                if len(parts) == 3:\n",
    "                    data.append([parts[0], parts[1], parts[2]])\n",
    "                else:\n",
    "                    data.append([parts[0], np.nan, np.nan])\n",
    "            df = pd.DataFrame(data, columns=headers)\n",
    "            df[headers[1]] = pd.to_numeric(df[headers[1]], errors='coerce')\n",
    "            df[headers[2]] = pd.to_numeric(df[headers[2]], errors='coerce')\n",
    "            dataframes.append(df)\n",
    "        return dataframes\n",
    "\n",
    "    def calculate_statistics_ordered(dataframes):\n",
    "        metrics = dataframes[0]['Metric']\n",
    "        result_list = []\n",
    "        for metric in metrics:\n",
    "            metric_data = [df[df['Metric'] == metric] for df in dataframes]\n",
    "            metric_stats = {}\n",
    "            for col in ['Training', 'Validation']:\n",
    "                valid_data = pd.concat([md[col] for md in metric_data]).dropna()\n",
    "                if not valid_data.empty:\n",
    "                    mean_val = valid_data.mean()\n",
    "                    std_val = valid_data.std()\n",
    "                    metric_stats[col] = (mean_val, std_val)\n",
    "                else:\n",
    "                    metric_stats[col] = np.nan\n",
    "            result_list.append((metric, metric_stats))\n",
    "        return result_list\n",
    "\n",
    "    def format_output_ordered(statistics_ordered):\n",
    "        output_df = pd.DataFrame(columns=['Metric', 'Training', 'Validation'])\n",
    "        for metric, values in statistics_ordered:\n",
    "            row = {'Metric': metric}\n",
    "            for col in ['Training', 'Validation']:\n",
    "                if col in values and not pd.isna(values[col]):\n",
    "                    row[col] = f\"({values[col][0]:.3f}, {values[col][1]:.3f})\"\n",
    "                else:\n",
    "                    row[col] = np.nan\n",
    "            output_df = pd.concat([output_df, pd.DataFrame([row])], ignore_index=True)\n",
    "        output_string = output_df.to_string(index=False)\n",
    "        return output_string\n",
    "\n",
    "    # Process the input tables\n",
    "    dataframes = parse_tables_manual(metric_tables)\n",
    "    statistics = calculate_statistics_ordered(dataframes)\n",
    "    output_table = format_output_ordered(statistics)\n",
    "    return output_table\n",
    "# Example usage\n",
    "metric_tables = [\n",
    "    \"\"\"                              Metric  Training  Validation\n",
    "0                   Average Accuracy     0.624       0.576\n",
    "1     Standard Deviation of Accuracy     0.242       0.203\n",
    "2                      Best Accuracy     0.944       0.803\n",
    "3                      Last Accuracy     0.944       0.803\n",
    "4                        Overall AUC       NaN       0.464\n",
    "5                   Maximum F1 Score       NaN       0.809\n",
    "6                       Minimum Loss       NaN       0.650\n",
    "7  Difference in Average Loss Last N       NaN       0.420\n",
    "8  Standard Deviation of Loss Last N       NaN       0.093\"\"\",\n",
    "\n",
    "\n",
    "\"\"\"                              Metric  Training  Validation\n",
    "0                   Average Accuracy     0.339       0.326\n",
    "1     Standard Deviation of Accuracy     0.005       0.004\n",
    "2                      Best Accuracy     0.341       0.337\n",
    "3                      Last Accuracy     0.341       0.327\n",
    "4                        Overall AUC       NaN       0.518\n",
    "5                   Maximum F1 Score       NaN       0.000\n",
    "6                       Minimum Loss       NaN       1.099\n",
    "7  Difference in Average Loss Last N       NaN      -0.001\n",
    "8  Standard Deviation of Loss Last N       NaN       0.000\"\"\",\n",
    "\n",
    "\n",
    "\"\"\"                              Metric  Training  Validation\n",
    "0                   Average Accuracy     0.344       0.334\n",
    "1     Standard Deviation of Accuracy     0.004       0.018\n",
    "2                      Best Accuracy     0.361       0.414\n",
    "3                      Last Accuracy     0.337       0.323\n",
    "4                        Overall AUC       NaN       0.547\n",
    "5                   Maximum F1 Score       NaN       0.000\n",
    "6                       Minimum Loss       NaN       1.090\n",
    "7  Difference in Average Loss Last N       NaN      -0.002\n",
    "8  Standard Deviation of Loss Last N       NaN       0.002\"\"\", \n",
    "\n",
    "\"\"\"                              Metric  Training  Validation\n",
    "0                   Average Accuracy     0.819       0.697\n",
    "1     Standard Deviation of Accuracy     0.176       0.111\n",
    "2                      Best Accuracy     0.966       0.778\n",
    "3                      Last Accuracy     0.960       0.773\n",
    "4                        Overall AUC       NaN       0.508\n",
    "5                   Maximum F1 Score       NaN       0.778\n",
    "6                       Minimum Loss       NaN       0.836\n",
    "7  Difference in Average Loss Last N       NaN       1.190\n",
    "8  Standard Deviation of Loss Last N       NaN       0.182\"\"\",\n",
    "\n",
    "\"\"\"                              Metric  Training  Validation\n",
    "0                   Average Accuracy     0.367       0.392\n",
    "1     Standard Deviation of Accuracy     0.027       0.056\n",
    "2                      Best Accuracy     0.412       0.480\n",
    "3                      Last Accuracy     0.401       0.465\n",
    "4                        Overall AUC       NaN       0.489\n",
    "5                   Maximum F1 Score       NaN       0.010\n",
    "6                       Minimum Loss       NaN       1.066\n",
    "7  Difference in Average Loss Last N       NaN      -0.006\n",
    "8  Standard Deviation of Loss Last N       NaN       0.013\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # More tables would follow here\n",
    "]\n",
    "\n",
    "result = process_metric_tables(metric_tables)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% Increase AUC: -3.81\n",
      "% Increase Best Accuracy: -10.794\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "\n",
    "def parse_metric_from_table(table, metric_name):\n",
    "    \"\"\"\n",
    "    Parses a specified metric and its values from a string representation of a table.\n",
    "    \n",
    "    Parameters:\n",
    "    - table (str): Multi-line string containing the table of metrics.\n",
    "    - metric_name (str): Name of the metric to parse (e.g., 'Overall AUC', 'Best Accuracy').\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple[float, float]: (value_standard, value_augmented) from the Training and Validation columns if available.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(rf\"{re.escape(metric_name)}\\s+.*?\\s+\\((\\d+\\.\\d+),\\s*\\d+\\.\\d+\\)\")\n",
    "    match = pattern.search(table)\n",
    "    if match:\n",
    "        return (None, float(match.group(1)))  # Only validation values are returned, as training values are not needed\n",
    "    return (None, None)\n",
    "\n",
    "def calculate_percentage_increase(standard, augmented):\n",
    "    \"\"\"\n",
    "    Calculates the percentage increase from the standard dataset to the augmented dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - standard (float): Metric value from the standard dataset.\n",
    "    - augmented (float): Metric value from the augmented dataset.\n",
    "    \n",
    "    Returns:\n",
    "    - float: Percentage increase of the metric, rounded to three decimal places.\n",
    "    \"\"\"\n",
    "    if standard is not None and augmented is not None and standard != 0:\n",
    "        return round(((augmented - standard) / standard) * 100, 3)\n",
    "    return None\n",
    "\n",
    "def write_results_to_csv(path, method, weather, percentage_auc, percentage_accuracy):\n",
    "    \"\"\"\n",
    "    Writes the calculated results to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - method (str): Description of the method (e.g., \"Standard\", \"Augmented\").\n",
    "    - weather (str): Description of the weather condition (e.g., \"Sunny\", \"Cloudy\").\n",
    "    - type_ (str): Type of data or experiment (e.g., \"Simulation\", \"Real-World\").\n",
    "    - percentage_auc (float): Calculated percentage increase for AUC.\n",
    "    - percentage_accuracy (float): Calculated percentage increase for Best Accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(path, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([method, weather, percentage_auc, percentage_accuracy])\n",
    "\n",
    "\n",
    "# Example string tables\n",
    "table1 = \"\"\"                           Metric       Training     Validation\n",
    "                 Average Accuracy (0.508, 0.209) (0.491, 0.159)\n",
    "   Standard Deviation of Accuracy (0.103, 0.107) (0.097, 0.068)\n",
    "                    Best Accuracy (0.633, 0.280) (0.630, 0.173)\n",
    "                    Last Accuracy (0.608, 0.303) (0.550, 0.204)\n",
    "                      Overall AUC            NaN (0.525, 0.026)\n",
    "                 Maximum F1 Score            NaN (0.392, 0.406)\n",
    "                     Minimum Loss            NaN (0.938, 0.203)\n",
    "Difference in Average Loss Last N            NaN (0.316, 0.483)\n",
    "Standard Deviation of Loss Last N            NaN (0.065, 0.093)\"\"\"\n",
    "\n",
    "table2 = \"\"\"                           Metric       Training     Validation\n",
    "                 Average Accuracy (0.499, 0.215) (0.465, 0.164)\n",
    "   Standard Deviation of Accuracy (0.091, 0.111) (0.078, 0.081)\n",
    "                    Best Accuracy (0.605, 0.321) (0.562, 0.214)\n",
    "                    Last Accuracy (0.597, 0.325) (0.538, 0.235)\n",
    "                      Overall AUC            NaN (0.505, 0.031)\n",
    "                 Maximum F1 Score            NaN (0.319, 0.433)\n",
    "                     Minimum Loss            NaN (0.948, 0.199)\n",
    "Difference in Average Loss Last N            NaN (0.320, 0.520)\n",
    "Standard Deviation of Loss Last N            NaN (0.058, 0.079)\"\"\"\n",
    "\n",
    "# Use these functions to extract and calculate percentage increases\n",
    "auc_values1 = parse_metric_from_table(table1, \"Overall AUC\")\n",
    "auc_values2 = parse_metric_from_table(table2, \"Overall AUC\")\n",
    "\n",
    "best_accuracy_values1 = parse_metric_from_table(table1, \"Best Accuracy\")\n",
    "best_accuracy_values2 = parse_metric_from_table(table2, \"Best Accuracy\")\n",
    "\n",
    "method = \"contrast(2)\"\n",
    "weather = \"45\"\n",
    "\n",
    "percentage_increase_auc = calculate_percentage_increase(auc_values1[1], auc_values2[1])\n",
    "percentage_increase_best_accuracy = calculate_percentage_increase(best_accuracy_values1[1], best_accuracy_values2[1])\n",
    "\n",
    "# Write to CSV\n",
    "path = \"../results/round1/percent_increase.csv\"\n",
    "\n",
    "write_results_to_csv(path, method, weather, percentage_increase_auc, percentage_increase_best_accuracy)\n",
    "\n",
    "print(\"% Increase AUC:\", percentage_increase_auc)\n",
    "print(\"% Increase Best Accuracy:\", percentage_increase_best_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
