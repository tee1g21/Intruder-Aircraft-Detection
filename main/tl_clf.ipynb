{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 05:28:05.423515: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-19 05:28:06.021484: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import dataset as ds\n",
    "import config as cfg\n",
    "from evaluate import Evaluate\n",
    "import tools\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Rescaling, Dropout\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "from clearml import Task\n",
    "import clearml\n",
    "clearml.browser_login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClearML\n",
    "https://app.clear.ml/dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base paths for the images and labels\n",
    "train_images_path = f'{cfg.BASE_DATASET}/images/train'\n",
    "train_labels_path = f'{cfg.BASE_DATASET}/labels/train'\n",
    "val_images_path = f'{cfg.BASE_DATASET}/images/valid'\n",
    "val_labels_path = f'{cfg.BASE_DATASET}/labels/valid'\n",
    "\n",
    "# Base path for metadata\n",
    "metadata_path = f'{cfg.BASE_DATASET}/metadata.json'\n",
    "\n",
    "# Create the DataFrames for the train and validation sets\n",
    "train_df = ds.create_dataframe(train_images_path, train_labels_path, metadata_path)\n",
    "valid_df = ds.create_dataframe(val_images_path, val_labels_path, metadata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name= cfg.CLF_PROJECT_NAME + '/epoch_test'\n",
    "dataset_name = 'test250'\n",
    "dataset_dir = cfg.CLF_DATASET_DIR + f'/{dataset_name}'\n",
    "project_dir = f'{cfg.CLF_PROJECT_DIR}/{dataset_name}/'\n",
    "class_names = cfg.CLF_CLASS_NAMES\n",
    "\n",
    "zoom_factor = 1.5\n",
    "\n",
    "epochs = 50\n",
    "N = 20\n",
    "\n",
    "RUN = 5\n",
    "\n",
    "\n",
    "task_name = 'epoch_test'\n",
    "task_name = f'{task_name}-{epochs}-{RUN}'\n",
    "\n",
    "\n",
    "#augmentation_metadata = {\n",
    "#    'methods': {        \n",
    "#        'bnc': {\n",
    "#            'parameters': {\n",
    "#                'alpha': 0.5,\n",
    "#                'beta': 0\n",
    "#            },\n",
    "#            'apply_to_percentage': 1.5  # 50% of the training images\n",
    "#        }        \n",
    "#    }\n",
    "#}\n",
    "\n",
    "augmentation_metadata = {\n",
    "        'methods': {        \n",
    "            'histEq': {\n",
    "                'parameters': {\n",
    "                    'p': 1.0  # Probability of applying the augmentation\n",
    "                },\n",
    "                'apply_to_percentage': 2.0  # 50% of the training images\n",
    "            }       \n",
    "        }\n",
    "    }\n",
    "\n",
    "train_size = 250\n",
    "val_size = int(train_size * 0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomev\\AppData\\Local\\Temp\\ipykernel_13044\\2678951675.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_train_df['stratify_key'] = filtered_train_df['ac'] + '_' + filtered_train_df['weather'].astype(str)\n",
      "C:\\Users\\tomev\\AppData\\Local\\Temp\\ipykernel_13044\\2678951675.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_valid_df['stratify_key'] = filtered_valid_df['ac'] + '_' + filtered_valid_df['weather'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing dataset if pre-existing\n",
      "Copying training files:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 100%|██████████| 500/500 [00:00<00:00, 2052.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying validation files:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 100%|██████████| 100/100 [00:00<00:00, 2438.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'test250' created at C:\\github\\Third-Year-Project\\Intruder-Aircraft-Detection\\datasets\\Custom\\test250\n",
      "Processing train labels in test250:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing labels: 100%|██████████| 250/250 [00:01<00:00, 240.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing valid labels in test250:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing labels: 100%|██████████| 50/50 [00:00<00:00, 266.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label correction completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing existing directories/files: 100%|██████████| 3/3 [00:00<00:00, 5997.57it/s]\n",
      "Copying files: 100%|██████████| 250/250 [00:00<00:00, 1184.57it/s]\n",
      "Copying files: 100%|██████████| 250/250 [00:00<00:00, 1420.96it/s]\n",
      "Applying augmentations: 100%|██████████| 500/500 [00:09<00:00, 51.87it/s]\n"
     ]
    }
   ],
   "source": [
    "seed_time = tools.generate_seed()\n",
    "\n",
    "# fitler by weather\n",
    "\n",
    "# filter by location \n",
    "#filtered_train_df = train_df[(train_df['location'] == 'Boston')]\n",
    "#filtered_valid_df = valid_df[(train_df['location'] == 'Boston')]\n",
    "#\n",
    "train_df = train_df[(train_df['location'] == 'Palo Alto')]\n",
    "valid_df = valid_df[(valid_df['location'] == 'Palo Alto')]\n",
    "\n",
    "filtered_train_df = train_df[(train_df['weather'] == 2) | (train_df['weather'] == 3)]\n",
    "filtered_valid_df = valid_df[(valid_df['weather'] == 2) | (valid_df['weather'] == 3)]\n",
    "\n",
    "#\n",
    "#filtered_train_df = train_df[(train_df['location'] == 'Reno Tahoe')]\n",
    "#filtered_valid_df = valid_df[(train_df['location'] == 'Reno Tahoe')]\n",
    "#\n",
    "#filtered_train_df = train_df[(train_df['location'] == 'Osh Kosh')]\n",
    "#filtered_valid_df = valid_df[(train_df['location'] == 'Osh Kosh')]\n",
    "\n",
    "# Create a combined stratification key\n",
    "filtered_train_df['stratify_key'] = filtered_train_df['ac'] + '_' + filtered_train_df['weather'].astype(str)\n",
    "filtered_valid_df['stratify_key'] = filtered_valid_df['ac'] + '_' + filtered_valid_df['weather'].astype(str)\n",
    "\n",
    "# Now use this stratification key in train_test_split\n",
    "_, test_train_df = train_test_split(\n",
    "    filtered_train_df,\n",
    "    test_size=train_size,  # Number of items you want in your sample\n",
    "    stratify=filtered_train_df['stratify_key'],  # Stratify based on the combined column\n",
    "    random_state=seed_time  # Ensures reproducibility\n",
    ")\n",
    "\n",
    "_, test_val_df = train_test_split(\n",
    "    filtered_valid_df,\n",
    "    test_size=val_size,  # Number of items you want in your sample\n",
    "    stratify=filtered_valid_df['stratify_key'],  # Stratify based on the combined column\n",
    "    random_state=seed_time  # Ensures reproducibility\n",
    ")\n",
    "\n",
    "# create dataset directory from dataframes above\n",
    "ds.create_sub_dataset(dataset_dir, test_train_df, test_val_df, class_names)\n",
    "\n",
    "\n",
    "# correct single class labels to accomodate for multi-class classification\n",
    "ds.correct_dataset_labels(dataset_dir, test_train_df, test_val_df, class_names)\n",
    "\n",
    "# augment dataset\n",
    "ds.augment_dataset(dataset_dir, augmentation_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing to AID classification (apply zoom factor to all images)\n",
    "ds.pre_process_dataset_for_classification(dataset_dir, zoom_factor)\n",
    "\n",
    "# create class folders within train and valid directories for keras format\n",
    "ds.reorganize_dataset_for_keras(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dir = os.path.join(dataset_dir,'images','train')\n",
    "train_aug_dir = os.path.join(dataset_dir,'images','train-aug')\n",
    "valid_dir = os.path.join(dataset_dir,'images','valid')\n",
    "\n",
    "batch_size = 32 \n",
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "# load datasets using keras\n",
    "train_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True)\n",
    "\n",
    "train_aug_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_aug_dir,\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    "    image_size=(img_height, img_width),\n",
    "    shuffle=True)\n",
    "\n",
    "valid_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    valid_dir,\n",
    "    batch_size=batch_size,\n",
    "    seed=42,\n",
    "    image_size=(img_height, img_width),    \n",
    "    shuffle=True)\n",
    "\n",
    "\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "hyper_params = {\n",
    "    'dataset': dataset_name,\n",
    "    'original_dataset_size': {'train_size': train_size, 'val_size': val_size},\n",
    "    'train_size': tools.count_images(train_dir),\n",
    "    'train_aug_size': tools.count_images(train_aug_dir),\n",
    "    'valid_size': tools.count_images(valid_dir),\n",
    "    'epochs': epochs, \n",
    "    'N': N,\n",
    "    'zoom_factor': zoom_factor, \n",
    "    'batch_size': batch_size, \n",
    "    'img_height': img_height, \n",
    "    'img_width': img_width, \n",
    "    'class_names': class_names\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test visualise \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_aug_data.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labels to one-hot encoding to allow for more metrics to be tracked\n",
    "def one_hot_enc(image, label):\n",
    "    return image, tf.one_hot(label, len(class_names))\n",
    "\n",
    "train_data = train_data.map(one_hot_enc)\n",
    "train_aug_data = train_aug_data.map(one_hot_enc)\n",
    "valid_data = valid_data.map(one_hot_enc)\n",
    "\n",
    "# check one hot new shape\n",
    "for image_batch, labels_batch in train_data:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data pipelines for train and val: \n",
    "# cache datasets to reduce I/O operations \n",
    "# shuffle training data for robustness\n",
    "# AUTOTUNE to dynamically optimize prefetching\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_data = train_data.cache().shuffle(1000, seed=42).prefetch(buffer_size=AUTOTUNE)\n",
    "train_aug_data = train_aug_data.cache().shuffle(1000, seed=42).prefetch(buffer_size=AUTOTUNE)\n",
    "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of validation labels (y_true) for evaluation\n",
    "validation_labels = []\n",
    "for images, labels in valid_data:\n",
    "    validation_labels.append(labels.numpy())\n",
    "\n",
    "validation_labels = np.concatenate(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN for image classification\n",
    "def get_model(): \n",
    "  # number of classes\n",
    "  num_classes = len(class_names) \n",
    "  model = Sequential([\n",
    "    Input(shape=(img_height, img_width, 3)), # inputs shape, height, width, channels (RGB)\n",
    "    Rescaling(1./255), # normalize pixel values\n",
    "    Conv2D(16, 3, padding='same', activation='relu'), # 16 filters, 3x3 kernel, relu activation\n",
    "    MaxPooling2D(), # performs 2D max pooling\n",
    "    Conv2D(32, 3, padding='same', activation='relu'), # 32 filters, 3x3 kernel, relu activation\n",
    "    MaxPooling2D(), # 2D max pooling\n",
    "    Conv2D(64, 3, padding='same', activation='relu'), # 64 filters, 3x3 kernel, relu activation\n",
    "    MaxPooling2D(), # 2D max pooling\n",
    "    Dropout(0.2), # dropout layer to increase regularization and reduce overfitting\n",
    "    Flatten(), # flattens output of previos layer to 1D\n",
    "    Dense(128, activation='relu'),      \n",
    "    Dense(num_classes, activation='softmax') \n",
    "  ])\n",
    "  \n",
    "  # compile model  \n",
    "  model.compile(optimizer='adam', # use Adam optimizer\n",
    "                loss='categorical_crossentropy', # use categorical crossentropy loss for mutliclass classification\n",
    "                metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]) # trach accuracy, precision, recall (and loss)\n",
    "\n",
    "  # print model summary\n",
    "  #model.summary()\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Pure\n",
    "\n",
    "# local logs directory\n",
    "logs_dir=cfg.CLF_PROJECT_DIR\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "\n",
    "#connect to clearml\n",
    "task = Task.init(project_name=project_name, task_name=f\"{task_name}-pure\")\n",
    "logger = task.get_logger()\n",
    "\n",
    "# clearml hyperparameters\n",
    "task.connect(hyper_params)\n",
    "\n",
    "# ensure model from scratch and get model\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "# train model\n",
    "print('Training model...')\n",
    "train_hst = model.fit(\n",
    "    train_data, \n",
    "    epochs=hyper_params['epochs'], \n",
    "    validation_data=valid_data,\n",
    "    callbacks=[tensorboard_callback]\n",
    "    )\n",
    "\n",
    "# predict on validation set\n",
    "print('Predicting on validation set...')\n",
    "y_pred = model.predict(valid_data)\n",
    "\n",
    "# evaluate\n",
    "pure_eval = Evaluate(train_hst, validation_labels, y_pred, class_names, aug=False, sf=3)\n",
    "\n",
    "# send metrics to clearML\n",
    "pure_eval.log_metrics(task, logger, N, None, hyper_params)\n",
    "\n",
    "\n",
    "# close task\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Augmented\n",
    "\n",
    "# local logs directory, callbacks allow for real time metrics in clearML\n",
    "logs_dir=cfg.CLF_PROJECT_DIR\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "\n",
    "#connect to clearml\n",
    "task = Task.init(project_name=project_name, task_name=f\"{task_name}-aug\")\n",
    "logger = task.get_logger()\n",
    "\n",
    "\n",
    "# clearml hyperparameters\n",
    "task.connect(hyper_params)\n",
    "\n",
    "# ensure model from scratch and get model\n",
    "model_aug = get_model()\n",
    "\n",
    "# train model\n",
    "print('Training model...')\n",
    "train_aug_hst = model_aug.fit(\n",
    "    train_aug_data, \n",
    "    epochs=hyper_params['epochs'], \n",
    "    validation_data=valid_data, \n",
    "    callbacks=[tensorboard_callback]\n",
    "    )\n",
    "\n",
    "# predict on validation set\n",
    "print('Predicting on validation set...')\n",
    "y_pred_aug = model_aug.predict(valid_data)\n",
    "\n",
    "# evaluate\n",
    "aug_eval = Evaluate(train_aug_hst, validation_labels, y_pred_aug, class_names, aug=True, sf=3)\n",
    "\n",
    "# send metrics to clearML\n",
    "aug_eval.log_metrics(task, logger, N, augmentation_metadata, hyper_params) \n",
    " \n",
    "# close task\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "All metrics have been fully logged in ClearML, this section is just for quick performance metrics not complete evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pure accuracy\n",
    "\n",
    "#pure_eval.average_accuracy(True)\n",
    "#print()\n",
    "#pure_eval.std_accuracy(True)\n",
    "#print()\n",
    "pure_eval.best_accuracy(True)\n",
    "print()\n",
    "#pure_eval.last_accuracy(True)\n",
    "#print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmented accuracy\n",
    "\n",
    "#aug_eval.average_accuracy(True)\n",
    "#print()\n",
    "#aug_eval.std_accuracy(True)\n",
    "#print()\n",
    "aug_eval.best_accuracy(True)\n",
    "print()\n",
    "#aug_eval.last_accuracy(True)\n",
    "#print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area under the ROC curve \n",
    "pure_eval.overall_auc(True)\n",
    "aug_eval.overall_auc(True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max validation F1 score \n",
    "pure_eval.max_f1(True)\n",
    "aug_eval.max_f1(True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "\n",
    "# minimum vakidation loss\n",
    "#pure_eval.min_loss(True)\n",
    "#aug_eval.min_loss(True)\n",
    "#print()\n",
    "\n",
    "# difference in average loss between training and validation sets for the last N epochs\n",
    "pure_eval.diff_avg_loss_lastN(N,True)\n",
    "aug_eval.diff_avg_loss_lastN(N,True)\n",
    "print()\n",
    "\n",
    "# standard deviation of loss for the last N epochs\n",
    "#pure_eval.std_loss_lastN(N,True)\n",
    "#aug_eval.std_loss_lastN(N,True)\n",
    "#print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Specific Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pure class auc\n",
    "\n",
    "#pure_eval.confusion_matrix_class_report(False)\n",
    "#print()\n",
    "\n",
    "pure_eval.class_auc(True)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmented class auc\n",
    "#aug_eval.confusion_matrix_class_report(True)\n",
    "#print()\n",
    "\n",
    "aug_eval.class_auc(True)    \n",
    "print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
